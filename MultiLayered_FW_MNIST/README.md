## üîß Technical Showcase
- **Algorithm Implementation**: Building ML models from scratch
- **Data Handling**: Preprocessing, normalization, visualization
- **Model Evaluation**: Accuracy metrics, confusion matrices, analysis
- **Problem Solving**: Debugging, optimization, performance tuning
- **Skills**: Python, NumPy, ML Algorithms, Data Analysis, Model Evaluation

## üöÄ Projects
### MNIST Digit Classification: Perceptron Implementation with 85%+ Accuracy
- **Description**: Multi-layer perceptrons (1-3 layers) trained from scratch on MNIST dataset. Experiments with Tanh/ReLU activations, L2 regularization, and overfitting analysis. Achieved ~95% validation accuracy with 3-layer ReLU net.
- **Key Features**: Custom backprop, mini-batch SGD, accuracy plotting.
- **Notebook**: [mnist_perceptron.ipynb](MNIST-Perceptron/mnist_perceptron.ipynb)
- **Results**: [Accuracy Plots](MNIST-Perceptron/accuracy_plots.png)
- **Dataset**: [MNIST from Microsoft AI Curriculum](https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz)

### [Future Labs to Be Added...]
- e.g., Linear Regression, Neural Nets on Iris, etc.

## üìä Results Summary
| Model | Layers | Activation | Val Accuracy | Overfitting Gap |
|-------|--------|------------|--------------|-----------------|
| 1-Layer | 1 | Tanh | ~88% | 0.02 |
| 2-Layer | 2 | ReLU | ~93% | 0.01 |
| 3-Layer | 3 | ReLU | ~95% | 0.01 |

## How to Run
1. Clone: `git clone https://github.com/yourusername/AI-ML-Portfolio.git`
2. Install: `pip install -r requirements.txt`
3. Download MNIST: Run notebook cell for `wget`.
4. Jupyter: `jupyter notebook MNIST-Perceptron/mnist_perceptron.ipynb`

## License
MIT License‚Äîfeel free to fork and build on it!

---
Built with ‚ù§Ô∏è by Carlos Berlin Arifin | [LinkedIn](your-linkedin) | [Contact](your-email)
